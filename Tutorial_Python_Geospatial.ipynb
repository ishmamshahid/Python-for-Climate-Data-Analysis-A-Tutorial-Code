{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8uibPgNfmSo"
   },
   "source": [
    "By: Ali Ahmadalipour ([LinkedIn](https://www.linkedin.com/in/ahmadalipour/), [Twitter](https://twitter.com/hydroclimali))\n",
    "\n",
    "April 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJkxVBvljChL"
   },
   "source": [
    "Link to a LinkedIn article I wrote explaining the details: https://www.linkedin.com/pulse/python-climate-data-analysis-tutorial-code-ali-ahmadalipour/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-83tcwHa3O3u"
   },
   "source": [
    "***Basic info for running the code in a notebook environment:***\n",
    "+ To run a cell, press Shift + Enter\n",
    "+ To restart the analysis (i.e. clean the variables and RAM, but keep the downloaded data), restart the runtime from the top menu\n",
    "+ To completely start over (i.e. clean RAM and temporary storage that may contain downloaded data or any saved figure), go to Runtime>Manage sessions->TERMINATE.\n",
    "\n",
    "\n",
    "***Basic info about Google Colab Notebooks:***\n",
    "* If the page stays idle for several minutes, the runtime may be terminated, and you'll need to connect again.\n",
    "* Google provides you about 70GB free temporary disk space. Any data you download or any model output is by default saved temporarily in this terminal. Therefore, if the runtime is terminated, you'll lose that data. If you would like to keep the data or the outputs, you can connect to your Google drive and choose any specific directory there. Here's how to connect to your google drive:\n",
    "```\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1TCT7AhyiFj"
   },
   "source": [
    "------------\n",
    "------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOFhBVx_fQcY"
   },
   "source": [
    "# 1. Basic (download, extract & save data, concat, groupby, select):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EO7GABvfobh"
   },
   "source": [
    "In this section, we will download and analyze gridded precipitation data (from CPC). The goal is to extract daily data, find monthly totals, find spatial average of precipitation in a given domain, plot the results, and save the outputs as netcdf files.\n",
    "We will work with some of the commonly used functionalities of [xarray](https://docs.xarray.dev/en/stable/) (a powerful python library for analyzing geospatial data) such as:\n",
    "- open_mfdataset (in xarray, which opens multiple files at the same time)\n",
    "- concatenate datasets\n",
    "- groupby\n",
    "- slicing and selecting data\n",
    "- save as netcdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNtomODXlRJU",
    "outputId": "6dda2ddd-4cb5-4aa9-97c1-f4b2b5901cfd"
   },
   "outputs": [],
   "source": [
    "# Later in the advanced section of this tutorial (section 3.2), we will be analyzing \n",
    "# zarr data format, and the pre-installed xarray on google colab is not able to \n",
    "# do so. Thus, we need to intall the complete version of xarray to be able to do it.\n",
    "!pip install xarray[complete] # this may take a few seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f0GjsiKzISk",
    "outputId": "37225281-b950-4c72-b486-da65af1d67cd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaqscVCTzH_C"
   },
   "outputs": [],
   "source": [
    "for yr in range(2011,2015): # note that in python, the end range is not inclusive. So, in this case data for 2015 is not downloaded.\n",
    "    url = f'https://downloads.psl.noaa.gov/Datasets/cpc_us_precip/RT/precip.V1.0.{yr}.nc'\n",
    "    savename = url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(url,savename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9j4r-H32jkY"
   },
   "source": [
    "Let's start simple: open data for two years and concatenate them to one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UCk-MMiehgaB"
   },
   "outputs": [],
   "source": [
    "ds2011 = xr.open_dataset('precip.V1.0.2011.nc')\n",
    "ds2012 = xr.open_dataset('precip.V1.0.2012.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "HFMHxeQc316O",
    "outputId": "9bade575-d7d0-4d7e-ea47-cbae50ce5121"
   },
   "outputs": [],
   "source": [
    "ds2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "hvj00x0J31o0",
    "outputId": "ad5baa54-00f5-424c-c5f7-0060994e83cf"
   },
   "outputs": [],
   "source": [
    "ds2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zedQxXzo31YH"
   },
   "outputs": [],
   "source": [
    "ds2011_2012 = xr.concat([ds2011,ds2012], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "8cKvT1hP4HoV",
    "outputId": "58f8051b-a53e-4b85-a82d-001ec72c35d6"
   },
   "outputs": [],
   "source": [
    "ds2011_2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At8nvy9p4Ojp"
   },
   "source": [
    "Now, let's try something similar, but through a more efficient way (especially if the number of files are more than two):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CpVurg04HSk"
   },
   "outputs": [],
   "source": [
    "ds2011_2014 = xr.open_mfdataset('precip.V1.0.*.nc', concat_dim='time', combine='nested')\n",
    "# Or, you can use the following command to do the same thing:\n",
    "# ds2011_2014 = xr.open_mfdataset('precip*.nc', combine='by_coords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "QikTJZ2A5F53",
    "outputId": "16dc67f9-015e-423b-e1c6-4d94ac77c40f"
   },
   "outputs": [],
   "source": [
    "ds2011_2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1U9r2DCV5T6R"
   },
   "source": [
    "Now let's focus on 2012 and extract the monthly precipitation sum and make a simple plot of one of the months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "1I_gUeqs5Sak",
    "outputId": "ecb0075f-77d6-4312-be10-0348ac524549"
   },
   "outputs": [],
   "source": [
    "# The great thing about groupby is that you do not need to worry about the leap years or \n",
    "# number of days in each month.\n",
    "# In addition, xarray is label-aware and when you pass the plot function, it understands that you want to\n",
    "# make a spatial plot and finds the lat and lon values and the appropriate title and labels.\n",
    "ds2012_mon = ds2012.groupby('time.month').sum()\n",
    "ds2012_mon.precip[0,:,:].plot(cmap='jet', vmax=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G7UIXqKsjtG"
   },
   "source": [
    "The above plot is quite simple and not high quality (e.g. the areas outside the US boundary had no data and are all shown in dark blue, both x & y axis limits are a bit large and can be narrowed down, the title is not exactly what we may like, etc.). We will now develop a more personalized plot for all the 12 months as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeiXzSVY1gQM"
   },
   "outputs": [],
   "source": [
    "import calendar # We'll use this library to easily add month name to subplot titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufOghu1MuwCk"
   },
   "outputs": [],
   "source": [
    "# First, We will develop a land mask data array that we can use to mask out the nan values:\n",
    "landmask = ds2012.precip.sum(dim='time')>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "2NuVzshq5SQk",
    "outputId": "1348d1a1-cf08-444e-fed8-121ff9b5926a"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,8], facecolor='w')\n",
    "plt.subplots_adjust(bottom=0.15, top=0.96, left=0.04, right=0.99, \n",
    "                    wspace=0.2, hspace=0.27) # wspace and hspace adjust the horizontal and vertical spaces, respectively.\n",
    "nrows = 3\n",
    "ncols = 4\n",
    "for i in range(1, 13):\n",
    "    plt.subplot(nrows, ncols, i)\n",
    "    dataplot = ds2012_mon.precip[i-1, :, :].where(landmask) # Remember that in Python, the data index starts at 0, but the subplot index start at 1.\n",
    "    p = plt.pcolormesh(ds2012_mon.lon, ds2012_mon.lat, dataplot,\n",
    "                   vmax = 400, vmin = 0, cmap = 'nipy_spectral_r',\n",
    "                   ) \n",
    "    plt.xlim([233,295])\n",
    "    plt.ylim([25,50])\n",
    "    plt.title(calendar.month_name[dataplot.month.values], fontsize = 13, \n",
    "              fontweight = 'bold', color = 'b')\n",
    "    plt.xticks(fontsize = 11)\n",
    "    plt.yticks(fontsize = 11)\n",
    "    if i % ncols == 1: # Add ylabel for the very left subplots\n",
    "        plt.ylabel('Latitude', fontsize = 11, fontweight = 'bold')\n",
    "    if i > ncols*(nrows-1): # Add xlabel for the bottom row subplots\n",
    "        plt.xlabel('Longitude', fontsize = 11, fontweight = 'bold')\n",
    "\n",
    "# Add a colorbar at the bottom:\n",
    "cax = fig.add_axes([0.25, 0.06, 0.5, 0.018])\n",
    "cb = plt.colorbar(cax=cax, orientation='horizontal', extend = 'max',)\n",
    "cb.ax.tick_params(labelsize=11)\n",
    "cb.set_label(label='Precipitation (mm)', color = 'k', size=14)\n",
    "\n",
    "# Now we can save a high resolution (300dpi) version of the figure:\n",
    "plt.savefig('Fig_prec_cpc_mon_2012.png', format = 'png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EIo6Tq560s0"
   },
   "source": [
    "Now let's say we want to extract data for a specific boundary and look at the average condition within that area of interest. For simplicity, we can think of a rectangular box (but you can easily develop any landmask as above and use it to focus on only your domain of interest). For this case, let's look at a rectangular box almost similar to the state of Kansas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXPn9-TC60QO"
   },
   "outputs": [],
   "source": [
    "top = 40\n",
    "bottom = 37\n",
    "left = 258\n",
    "right = 265.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUyGJP236z7_"
   },
   "outputs": [],
   "source": [
    "ds_sel = ds2011_2014.isel(lon=(ds2011_2014.lon >= left) & (ds2011_2014.lon <= right),\n",
    "                          lat=(ds2011_2014.lat >= bottom) & (ds2011_2014.lat <= top),\n",
    "                          )\n",
    "ds_sel_avg = ds_sel.mean(dim=['lat','lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pf2nx6vF_aa7"
   },
   "source": [
    "Now let's plot the cumulative daily precipitation of the selected area for each year. To make things easier, let's drop Feb 29th from any leap years in the record. Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJQAUG5T-AJP"
   },
   "outputs": [],
   "source": [
    "ds_sel_avg_noleap = ds_sel_avg.sel(\n",
    "    time=~((ds_sel_avg.time.dt.month == 2) & (ds_sel_avg.time.dt.day == 29)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "uO-MeJ-w9_53",
    "outputId": "f7b3e354-146c-420d-e780-19df2a59f3f0"
   },
   "outputs": [],
   "source": [
    "# Here's how the result will look like:\n",
    "ds_sel_avg_noleap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x9NS33dFWds"
   },
   "outputs": [],
   "source": [
    "# Now we can easily save that output as a netcdf file using xarray:\n",
    "ds_sel_avg_noleap.to_netcdf('ds_prec_Kansas_noleap_2011_2014.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "3wwD49nyAoCY",
    "outputId": "11be1a94-569e-4fdb-d0d3-30284a0b44f9"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[8,5], facecolor='w')\n",
    "for yr in range(2011,2015):\n",
    "    da_yr = ds_sel_avg_noleap.isel(time = ds_sel_avg_noleap.time.dt.year==yr).precip\n",
    "    dataplot = da_yr.cumsum()\n",
    "    plt.plot(dataplot, linewidth=2, label = yr)\n",
    "plt.legend(fontsize=13)\n",
    "plt.grid()\n",
    "plt.xticks(fontsize=12) # we can also change the ticks to be on Jan-1, Feb-1, etc. but I'll skip it for here.\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylabel('Precipitation (mm)', fontsize = 13, fontweight = 'bold')\n",
    "plt.xlabel('Day of Year', fontsize = 13, fontweight = 'bold')\n",
    "plt.xlim([0,365])\n",
    "plt.ylim(bottom=0)\n",
    "plt.title('Annual cumulative precipitation in Kansas', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig_cumsum_prec_Kansas.png', format = 'png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MoGHcoKRyAc"
   },
   "source": [
    "We could also do a little more modification to revise the xticklabels and show the exact month and day values (instead of julian day number). See if you can figure it out yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fqAB1OEGI1l"
   },
   "source": [
    "-------------------------------------------------------------\n",
    "-------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJHLaWQzhfmT"
   },
   "source": [
    "# 2. Intermediate (interpolate, revise coordinates, rolling average):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfU1toD9yd23"
   },
   "source": [
    "For this section, we will dive deeper. We will be using two different datasets with different resolutions and we will work with interpolation. The two datasets that I have considered are CPC-Globe and gridMet minimum air temperature data. CPC-Globe has a 0.5 degree (~ 50km) spatial resolution, whereas the same for gridMET is 1/24 degree (~ 4km).\n",
    "Here's a summary of the main functionalities that we will be practicing in this section:\n",
    "+ Interpolation\n",
    "+ Converting 0:360 longitude axis to -180:180\n",
    "+ Scaling a dataset (e.g. convert degrees Kelvin to degrees Celcius)\n",
    "+ Rolling average (with any time window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TQ9jdwmSz6qE"
   },
   "outputs": [],
   "source": [
    "# I import the libraries again, to keep the examples separate from each other (in case someone wants to start from here).\n",
    "import urllib.request\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlaTrs1X0h0Z"
   },
   "source": [
    "Let's download the two datasets for 2021:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9ryOGRtz6aJ",
    "outputId": "e7ed2053-e1aa-45be-94cd-e05c15475364"
   },
   "outputs": [],
   "source": [
    "# Downloading GridMet tmin ():\n",
    "url = 'https://www.northwestknowledge.net/metdata/data/tmmn_2021.nc'\n",
    "savename = 'tmin_gridmet_2021.nc'\n",
    "urllib.request.urlretrieve(url, savename)\n",
    "\n",
    "# Downloading CPC-Globe tmin (0.5 deg, ~50km):\n",
    "url = 'https://downloads.psl.noaa.gov/Datasets/cpc_global_temp/tmin.2021.nc'\n",
    "savename = 'tmin_CPC_2021.nc'\n",
    "urllib.request.urlretrieve(url, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viEikJwPz6JU"
   },
   "outputs": [],
   "source": [
    "# Now lets open the two datasets and explore them:\n",
    "ds_gridmet = xr.open_dataset('tmin_gridmet_2021.nc')\n",
    "ds_CPC = xr.open_dataset('tmin_CPC_2021.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "KCOMjVSRz57M",
    "outputId": "3aae11b6-ce19-412d-b4ea-90e7658418a1"
   },
   "outputs": [],
   "source": [
    "ds_gridmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "4a3ElF5phdiU",
    "outputId": "22be84b4-af5f-4fa8-bb81-5553839550a5"
   },
   "outputs": [],
   "source": [
    "ds_CPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "7NY0kAU_DHPs",
    "outputId": "874b558a-4bfc-4d66-afec-6fb7354b1e3a"
   },
   "outputs": [],
   "source": [
    "# Let's plot the original data for Jan 1st:\n",
    "fig = plt.figure(figsize = [13,4.5])\n",
    "plt.subplot(1,2,1)\n",
    "ds_gridmet.air_temperature[0,:,:].plot(cmap = 'jet')\n",
    "plt.title('GridMet')\n",
    "plt.subplot(1,2,2)\n",
    "ds_CPC.tmin[0,:,:].plot(cmap = 'jet')\n",
    "plt.title('CPC-Globe')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlF2rnGb4Fq-"
   },
   "source": [
    "Looking at the two datasets, we see that there are a few differences that should be addressed:\n",
    "+ In the gridmet dataset, the \"crs\" coordinate can be dropped, and the \"day\" coordinate can be renamed to \"time\" to be consistent with the CPC dataset (similarly for \"air_temperature\").\n",
    "+ The gridmet data is in Kelvin, but CPC is in Celcius. Let's convert gridmet data to Celcius.\n",
    "+ In addition, the lon coordinate in one dataset is 0:360 and -180:180 in the other one. Let's change that to 0:360 for the gridmet data.\n",
    "+ Lastly, let's interpolate the coarser resolution data (CPC-Globe) to the gridmet resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFus-HVU3-AS"
   },
   "outputs": [],
   "source": [
    "ds_gridmet_revised = ds_gridmet.drop('crs').rename({'day':'time', 'air_temperature':'tmin'})\n",
    "ds_gridmet_revised = ds_gridmet_revised-273.15 # Convert Kelvin to Celcius\n",
    "lon_revised = ds_gridmet.lon + (ds_gridmet.lon < 0)*360\n",
    "ds_gridmet_revised = ds_gridmet_revised.assign_coords(lon = lon_revised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "nV7PVLQ93-PJ",
    "outputId": "07dae478-064a-48da-8aed-f823e262072b"
   },
   "outputs": [],
   "source": [
    "ds_gridmet_revised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IpAj-xtJ7Tzy"
   },
   "outputs": [],
   "source": [
    "ds_CPC_interp = ds_CPC.interp(lat = ds_gridmet_revised.lat, lon = ds_gridmet_revised.lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9dLvt_h78xQ"
   },
   "source": [
    "Great! We did all that with just a few lines of code. Please note that in interpolation, the data boundary (i.e. latlon bounds) are also matched to the output boundary.\n",
    "\n",
    "Now let's take a look at Feb 16, 2021 for the regions in Texas, where a severe cold storm happened. I did a quick analysis for that event last year and you can take a look at that in this link: https://www.linkedin.com/pulse/how-unusual-2021-texas-cold-span-ali-ahmadalipour/\n",
    "\n",
    "To make the temporal analyses easier, we first make sure that the \"time\" coordinate is in proper datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_ya8WBeAZ3h"
   },
   "outputs": [],
   "source": [
    "ds_gridmet_revised = ds_gridmet_revised.assign_coords(\n",
    "    time = pd.to_datetime(ds_gridmet_revised.time))\n",
    "ds_CPC_interp = ds_CPC_interp.assign_coords(\n",
    "    time = pd.to_datetime(ds_CPC_interp.time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCMjD55g9apG"
   },
   "outputs": [],
   "source": [
    "target_date = datetime.date(2021,2,16)\n",
    "target_date = pd.to_datetime(target_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "DBFWG6ea7TZ4",
    "outputId": "85b7895e-cbcb-4555-ee08-a0dff7ed55a2"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = [13,4.5], facecolor='w')\n",
    "plt.subplot(1,2,1)\n",
    "ds_gridmet_revised.sel(time=target_date).tmin.plot(cmap = 'nipy_spectral', vmin = -30, vmax = 20)\n",
    "plt.title(f'GridMet tmin on {target_date.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "ds_CPC_interp.sel(time=target_date).tmin.plot(cmap = 'nipy_spectral', vmin = -30, vmax = 20)\n",
    "plt.title(f'CPC-Globe tmin on {target_date.strftime(\"%Y-%m-%d\")}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtJ2MAqHE5NE"
   },
   "source": [
    "It can be seen that the interpolated data (CPC plot shown on right) does not necessarily have the details (specially the orographic and elevation effects) of the finer resolution data.\n",
    "\n",
    "Now let's find the spatial mean of both datasets around Austin, TX. Again, to make it simpler, I defined an estimate rectangular boundary that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6VaBsCJFtaj"
   },
   "outputs": [],
   "source": [
    "# Rough boundaries for Austin, TX:\n",
    "left = 360 - 97.9\n",
    "right = 360 - 97.6\n",
    "top = 30.5\n",
    "bottom = 30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l71IQzwoEDB8"
   },
   "outputs": [],
   "source": [
    "ds_Austin_gridmet = ds_gridmet_revised.isel(\n",
    "    lon=(ds_gridmet_revised.lon >= left) & (ds_gridmet_revised.lon <= right),\n",
    "    lat=(ds_gridmet_revised.lat >= bottom) & (ds_gridmet_revised.lat <= top),\n",
    ").mean(dim=['lat','lon'])\n",
    "ds_Austin_CPC = ds_CPC_interp.isel(\n",
    "    lon=(ds_CPC_interp.lon >= left) & (ds_CPC_interp.lon <= right),\n",
    "    lat=(ds_CPC_interp.lat >= bottom) & (ds_CPC_interp.lat <= top),\n",
    ").mean(dim=['lat','lon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6oWUzWbG-36"
   },
   "source": [
    "Now let's plot the two timeseries, but this time in degrees fahrenheit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "Ua_e-AlwIEOo",
    "outputId": "4ec2ef1a-ae8a-45f9-ec51-27e535133d4e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12,6])\n",
    "(ds_Austin_gridmet.tmin*1.8 + 32).plot(label = 'GridMet', color = 'r')\n",
    "(ds_Austin_CPC.tmin*1.8 + 32).plot(label = 'CPC', color = 'b')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(ticks = [datetime.date(2021,x,1) for x in range(1,13)], fontsize=12)\n",
    "plt.xlim([datetime.date(2021,1,1), datetime.date(2021,12,31)])\n",
    "plt.yticks(fontsize=13)\n",
    "plt.ylabel('Minimum air temperature (Tmin, °F)', fontsize = 12, \n",
    "           fontweight = 'bold')\n",
    "plt.xlabel('')\n",
    "plt.legend(fontsize=13, loc = 'upper left')\n",
    "plt.title('Austin, TX', fontsize=13, fontweight = 'bold')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GKD-rpZRIlI"
   },
   "source": [
    "As you can see, the two datasets are considerably different (more than 10°F) in late February and late October. Notably, daily temperature has a lot of noise (short-term variation). To have a smoother plot, let's calculate the moving average of tmin with a 15-day window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "bA17h2VRIEBU",
    "outputId": "45c9c643-8ef7-4e6e-bc1f-bc5cc71f1c10"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [12,6])\n",
    "(ds_Austin_gridmet.tmin*1.8 + 32).rolling(time=15,center=True).mean().plot(label = 'GridMet', color = 'r')\n",
    "(ds_Austin_CPC.tmin*1.8 + 32).rolling(time=15,center=True).mean().plot(label = 'CPC', color = 'b')\n",
    "plt.grid()\n",
    "plt.xticks(ticks = [datetime.date(2021,x,1) for x in range(1,13)], fontsize=12)\n",
    "plt.xlim([datetime.date(2021,1,1), datetime.date(2021,12,31)])\n",
    "plt.yticks(fontsize=13)\n",
    "plt.ylabel('Minimum air temperature (Tmin, °F)', fontsize = 12, \n",
    "           fontweight = 'bold')\n",
    "plt.xlabel('')\n",
    "plt.legend(fontsize=13, loc = 'upper left')\n",
    "plt.title('Austin, TX', fontsize=13, fontweight = 'bold')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUdA8OvLYq9O"
   },
   "source": [
    "Awesome! We are done with the intermediate example. You should now be able to replicate similar analyses for various datasets. There are a lot of other things that can be adjusted to make the plots more interesting. You can always search for anything you'd like to do and you will most likely find a decent answer for it on stackoverflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XXxuS7LieOO"
   },
   "source": [
    "# 3. Advanced (skip data download, anomaly calculation, timelapse animation):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAEi56Yh2sh9"
   },
   "source": [
    "## 3.1. Seasonal Forecast:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOJiubWr_zPT"
   },
   "source": [
    "In this part, we will analyze seasonal forecast data on the go (without downloading and saving the data on the disk). Then, we will look at calculating monthly anomaly (departure of each month from its historical mean state).\n",
    "\n",
    "The data that we will be focusing on is going to be the [NMME](https://www.cpc.ncep.noaa.gov/products/NMME/) seasonal climate prediction, which is a global dataset of 1 degree (~ 100km) spatial resolution and monthly temporal resolution with multiple months ahead forecast lead time. To make the analysis simpler, we will only focus on just one model (instead of the entire ensemble of available NMME models). Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmPwmmhSaXfa"
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbezeWgCKCC3"
   },
   "outputs": [],
   "source": [
    "url = 'http://iridl.ldeo.columbia.edu/SOURCES/.Models/.NMME/.GFDL-SPEAR/.FORECAST/.MONTHLY/.tref/dods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lDLNSZgDnND"
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(url,engine='netcdf4',decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "D9obvqwjDnDq",
    "outputId": "c3eb1a07-1753-40e5-a463-8423c971b350"
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GQ8xh4pK8Vg"
   },
   "source": [
    "The NMME coordinates are not really self-explanatory. So, here's an overview of what each coordinate stands for:\n",
    "+ S: Time (number of months since 1960-Jan-1)\n",
    "+ M: Ensemble member\n",
    "+ X: Longitude (in 0 0:360 format)\n",
    "+ L: Leadtime (in months; 0.5 indicating the current month, 1.5 being one month ahead, and so on)\n",
    "+ Y: Latitude\n",
    "+ *Z: this variable is only found in the GFDL model for temperature forecast, and it indicates that the data is at 2m height from ground.\n",
    "\n",
    "To make the data more descriptive and more convenient for analysis, we need to modify it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pwzaULlObcp"
   },
   "outputs": [],
   "source": [
    "ds = ds.rename({'S':'time', 'X':'lon', 'Y':'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rY0Bw1CDm3B"
   },
   "outputs": [],
   "source": [
    "start_date_NMME = pd.to_datetime(datetime.date(1960,1,1))\n",
    "time_new = [start_date_NMME + pd.DateOffset(months = x) for x in ds.time.values]\n",
    "ds = ds.assign_coords(time = time_new)\n",
    "if 'Z' in ds.dims:\n",
    "    ds = ds.squeeze(dim = 'Z').drop('Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "pfffc8rgDmfc",
    "outputId": "2bc3386b-22f7-4baa-8ad9-6a16293527ef"
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUTSfN3QRPCh"
   },
   "source": [
    "The above dataset has all the available forecast data for all leadtimes. We can now select our area of interest and limit the leadtime to our use case. For this example, let's take a look at the temperature forecast for Feb 2021 that was generated at the beginning of the same month (i.e. lead 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FuNlQHL4QZPL"
   },
   "outputs": [],
   "source": [
    "target_date = pd.to_datetime(datetime.date(2021,2,1))\n",
    "ds_sel = ds.sel(time=target_date).isel(L=0) \n",
    "# Note the difference use of \"sel\" and \"isel\". For the former, you should indicate \n",
    "# the exact value to be selected, but for the latter, the index should be specified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK1cCpBkTg6V"
   },
   "source": [
    "So far, the data is not loaded yet (although we can see all the metadata). To make the analysis easier, we will first load the data and then continue with the rest of analyses. For loading data, I am simply using .load(), but a better way of doing so is to use Dask and do the work in parallel mode. I won't go into that (partly because I tried it in Google Colab, and I was getting several errors here, and I didn't want to spend too much time on debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "qmS9-qeMSK1J",
    "outputId": "fabaea87-2769-4d19-c5cb-542f0f7face8"
   },
   "outputs": [],
   "source": [
    "ds_sel.load() # this can take a couple of minutes or so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJu-WxytkH4m"
   },
   "source": [
    "Now let's plot the data and explore it a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "1oaq-j-aSKrU",
    "outputId": "b6c7c8fc-61bd-4874-db98-f344fef29d28"
   },
   "outputs": [],
   "source": [
    "ds_sel.tref[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtpuq5ErkT32"
   },
   "source": [
    "Now let's calculate the standard deviation of forecasts among the ensemble members, and then plot them for the entire globe as well as only North America:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya5OGBFxSKZN"
   },
   "outputs": [],
   "source": [
    "ds_std = ds_sel.std(dim='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "MJdBqhQtSKQB",
    "outputId": "e45d6413-c43c-4bec-db44-4ef07d4dabfd"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,4.5])\n",
    "plt.subplot(1,2,1)\n",
    "ds_std.tref.plot(cmap = 'Spectral_r', vmin = 0, vmax = 4)\n",
    "plt.subplot(1,2,2)\n",
    "ds_std.tref.plot(cmap = 'Spectral_r', vmin = 0, vmax = 4)\n",
    "plt.xlim([230,300])\n",
    "plt.ylim([20,55])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1BdUPVzgYXY"
   },
   "source": [
    "Looking at the above plots, we can see that the uncertainty of temperature forecast in February 2021 is much higher across the northern latitudes (i.e. in winter) than the southern latitudes (i.e. in summer).\n",
    "\n",
    "Now let's check if the forecasts indicated any significant cold anomaly for Feb 2021 across the Midwest US and Southern Plains. To do so, we need to load the climatology (historical mean) of GFDL forecasts and then find the anomaly by subtracting it from forecasts. In other words:\n",
    "- Anomaly = Forecast - Climatology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjxK0E4KSJ6t"
   },
   "outputs": [],
   "source": [
    "ds_clim = xr.open_dataset('http://iridl.ldeo.columbia.edu/SOURCES/.Models/.NMME/.GFDL-SPEAR/.HINDCAST/.mc9120/.tref/dods',\n",
    "                          decode_times=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "mJH3T3nawhd-",
    "outputId": "e269ea72-60eb-43dd-9817-e28ff6d15038"
   },
   "outputs": [],
   "source": [
    "# Again, to make things easier, we first load the climatology into RAM:\n",
    "ds_clim.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpKuZlQayn2b"
   },
   "source": [
    "In the above dataset, S indicates the month index (0 corresponding to January and 11 indicating December). We should adjust the coordinates of this climatology dataset and match it to the \"ds_sel\" that we prepared above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctaB8IdgwhS0"
   },
   "outputs": [],
   "source": [
    "ds_clim = ds_clim.rename({'X':'lon', 'Y':'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRDNqpqPzZ13"
   },
   "outputs": [],
   "source": [
    "ds_clim_sel = ds_clim.isel(L = 0).sel(S = 1).drop(['S','L']) # S=1 for selecting February"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQe8itNOzSSt"
   },
   "outputs": [],
   "source": [
    "ds_anom = ds_sel - ds_clim_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_00P5Wq6whH2",
    "outputId": "b427b8d1-2aed-4099-f8b1-398b3a7f8397"
   },
   "outputs": [],
   "source": [
    "ds_anom.tref.plot(col = 'M', col_wrap = 10, vmin = -8, vmax = 8, cmap = 'bwr')\n",
    "plt.xlim([230,300])\n",
    "plt.ylim([20,55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2nod3y-39Zf"
   },
   "source": [
    "As it can be seen, most of the ensemble members of the GFDL-SPEAR model correctly forecasted negative anomaly (i.e. colder than usual condition) for the majority of Midwest US and Texas. Notably, this is the Lead-0 forecast, which was initiated at the beginning of Feb for the month of Feb (thus, basically a couple of weeks ahead). If we look back at lead-1 or beyond, such a strong pattern and consensus are not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "KPqHVv6Y3Pfl",
    "outputId": "0f391e71-7162-42af-faac-801092dff727"
   },
   "outputs": [],
   "source": [
    "# And here's the ensemble mean plot:\n",
    "ds_anom.mean(dim='M').tref.plot(cmap='bwr', vmax=5, vmin=-5)\n",
    "plt.xlim([230,300])\n",
    "plt.ylim([20,55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1CKrj-o0_hL"
   },
   "source": [
    "I was hoping to use two more libraries in this section:\n",
    "+ **dask** --> for parallel computing\n",
    "+ **cartopy** --> for customized mapping with geographic projection\n",
    "\n",
    "However, I ran into several errors trying to use them on Google Colab, so I decided to exclude them from the advanced section. But if you really want to reach an advanced proficiency level, I highly encourage you to try dask and cartopy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fm1w0R282yL9"
   },
   "source": [
    "## 3.2. Climate change assessment using CMIP6 data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuVWIWJ927xx"
   },
   "source": [
    "I thought it would be really useful to have a quick exercise for analyzing climate change data. In this part, we will explore CMIP6 air temperature projections from one model. We will look at monthly data for the historical period of 1970-2010 and future projections of SSP585 during 2010-2100.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1M8k-GQ5kb7",
    "outputId": "bdb500d4-2bc5-42b7-a0e6-4be0fd5bdb78"
   },
   "outputs": [],
   "source": [
    "!pip install gcsfs # this will take a few seconds. We need it to extract CMIP6 data from Google Cloud Storage.\n",
    "\n",
    "# We will be opening zarr data format, which is a relatively new data structure \n",
    "# that is practical for geospatial datasets. The pre-installed xarray on google\n",
    "# colab does not allow this. So, we need to intall the complete version of xarray.\n",
    "!pip install xarray[complete] # (adding this again in case someone wants to start from this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYVNqKY635DH"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6nQywFS342F"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "U4f37alQ34sX",
    "outputId": "f28ac096-4271-4c30-83a0-441309550cae"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-bk7-iH34jB",
    "outputId": "9c6e3575-2b82-4a0a-fc3d-12e5094844e7"
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBeu7XIt6LRU"
   },
   "source": [
    "The df table includes information for all climate change model runs that are available on Google Cloud Storage. As of March 2022, there are over half a million different model outputs (e.g. different models, variables, timescales, scenarios, ensemble member, etc.). Let's narrow down the outputs to only those of monthly air temperature in historical and ssp585 scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "Ajq6ytPE_Fhv",
    "outputId": "5e8e7413-b176-40fc-ea1c-7b1b879cfda3"
   },
   "outputs": [],
   "source": [
    "df_ssp585 = df.query(\"activity_id=='ScenarioMIP' & table_id == 'Amon' & \" +\\\n",
    "    \"variable_id == 'tas' & experiment_id == 'ssp585' & member_id == 'r1i1p1f1'\")\n",
    "print('Length of df_ssp585:', len(df_ssp585))\n",
    "df_ssp585.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "FswdqToK7HWi",
    "outputId": "9bbd9bba-2e44-49fa-bc4b-e88bac3dd86e"
   },
   "outputs": [],
   "source": [
    "df_historical = df.query(\"activity_id == 'CMIP' & table_id == 'Amon' & \" +\\\n",
    "    \"variable_id == 'tas' & experiment_id == 'historical' & member_id == 'r1i1p1f1'\")\n",
    "print('Length of df_historical:', len(df_historical))\n",
    "df_historical.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFwzDxM08CZr"
   },
   "source": [
    "Ok, much better. In summary, our CMIP6 search is narrowed down to 55 historical models and 35 ssp585 future monthly temperature \"forecasts\". In a decent climate change study, we should first evaluate the models during the historical period and then select a sub-set of models that outperform at the region of interest. However, since this is just an exercise, I will only choose one model that has data in both historical and future periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOiAM6jLA-b9"
   },
   "outputs": [],
   "source": [
    "model = 'GFDL-CM4'\n",
    "zstore_hist = df_historical.query(f\"source_id == '{model}'\").zstore.values[0]\n",
    "zstore_ssp585 = df_ssp585.query(f\"source_id == '{model}'\").zstore.values[0]\n",
    "gcs = gcsfs.GCSFileSystem(token='anon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GznVbPgC2XM"
   },
   "outputs": [],
   "source": [
    "mapper = gcs.get_mapper(zstore_hist)\n",
    "ds_hist = xr.open_zarr(mapper, consolidated = True)\n",
    "mapper = gcs.get_mapper(zstore_ssp585)\n",
    "ds_ssp585 = xr.open_zarr(mapper, consolidated = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "nGtgB6bTGerY",
    "outputId": "b7eee868-1105-4b25-a618-b2af606ef719"
   },
   "outputs": [],
   "source": [
    "ds_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CC03zDEGnfg"
   },
   "source": [
    "Looking at the metadata of the two datasets, the data periods can be beyond our interest (historical: 1850-2014, ssp585: 2015-2100). Therefore, we can select our period of interest and then load the subset of data for more convenience in further analyses. But before that, the time coordinate is in \"object\" format, which we need to convert to datetime to be able to easily analyze the timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NN7kexZJPz5",
    "outputId": "c48caec4-3eb0-456d-9230-aa8e3e1aa11f"
   },
   "outputs": [],
   "source": [
    "print('hist date range  :', ds_hist.time[0].values, ' , ', ds_hist.time[-1].values)\n",
    "print('ssp585 date range:', ds_ssp585.time[0].values, ' , ', ds_ssp585.time[-1].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxr9uHZFJMxu"
   },
   "outputs": [],
   "source": [
    "start_time = pd.to_datetime(datetime.date(1850,1,15)) # I chose 15 for all dates to make it easier.\n",
    "time_new_hist = [start_time + pd.DateOffset(months = x) for x in range(len(ds_hist.time))]\n",
    "\n",
    "start_time = pd.to_datetime(datetime.date(2015,1,15))\n",
    "time_new_ssp585 = [start_time + pd.DateOffset(months = x) for x in range(len(ds_ssp585.time))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAcV88E4L9XT"
   },
   "outputs": [],
   "source": [
    "ds_hist = ds_hist.assign_coords(time = time_new_hist)\n",
    "ds_ssp585 = ds_ssp585.assign_coords(time = time_new_ssp585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XTRiZsETC2Hk"
   },
   "outputs": [],
   "source": [
    "start_date = pd.to_datetime(datetime.date(1980,1,1))\n",
    "end_date = pd.to_datetime(datetime.date(2010,12,31))\n",
    "ds_hist_sel = ds_hist.isel(time=(ds_hist.time >= start_date) & (ds_hist.time <= end_date))\n",
    "\n",
    "start_date = pd.to_datetime(datetime.date(2070,1,1))\n",
    "end_date = pd.to_datetime(datetime.date(2099,12,31))\n",
    "ds_ssp585_sel = ds_ssp585.isel(time=(ds_ssp585.time >= start_date) & (ds_ssp585.time <= end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "f9EYcsMDC1--",
    "outputId": "81c231ff-3f20-4eb2-afd7-e6f54e4388b6"
   },
   "outputs": [],
   "source": [
    "ds_hist_sel.load()\n",
    "ds_ssp585_sel.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs0H0o68hIiZ"
   },
   "source": [
    "Great! Now let's take a look at the average monthly temperature change over the globe in distant future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcCctlSf7GZF"
   },
   "outputs": [],
   "source": [
    "tas_avg_hist = ds_hist_sel.groupby('time.month').mean()\n",
    "tas_avg_ssp585 = ds_ssp585_sel.groupby('time.month').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3zm5T2PgUt6"
   },
   "outputs": [],
   "source": [
    "tas_30yr_diff = tas_avg_ssp585 - tas_avg_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "id": "bfjI5fYshYGk",
    "outputId": "426637b9-963c-4d88-f67b-66a475f2d4ee"
   },
   "outputs": [],
   "source": [
    "tas_30yr_diff.tas.plot(col = 'month', col_wrap = 6, vmax = 10, vmin = 0, cmap = 'hot_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxAqAo0Yieun"
   },
   "source": [
    "And here's a plot for the annual mean change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "O71DWnqNiefn",
    "outputId": "92f53b9a-4e6c-4b58-e409-6277c7bc0497"
   },
   "outputs": [],
   "source": [
    "tas_30yr_diff.mean('month').tas.plot(figsize=[8,5], cmap = 'hot_r', \n",
    "                                     vmin = 0, vmax = 10)\n",
    "plt.title('Mean temperature change: 2070-2100 vs. 1980-2010')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NgamzchjQPe"
   },
   "source": [
    "The above plots show that in an extreme scenario (ssp585), the annual mean temperature in northern latitudes can be above 10°C warmer than the historical period, and the same for mid-latitudes can be ~5-7°C warmer. Please note that this is the result of one GCM realization at coarse resolution. In practice, these outputs should be first bias-corrected and downscaled before further analysis.\n",
    "\n",
    "Now, let's try an example and work with timeseries data (instead of 30-year mean). Let's look at the annual temperature change of each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9vyP8xenhc3"
   },
   "outputs": [],
   "source": [
    "tas_avg_hist_yr = tas_avg_hist.mean('month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TM3pmyPrnjz7"
   },
   "outputs": [],
   "source": [
    "tas_change_yr = ds_ssp585_sel.groupby('time.year').mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzLchR-lnvbf"
   },
   "outputs": [],
   "source": [
    "tas_change_yr = tas_change_yr - tas_avg_hist_yr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVSk4-Qc670s"
   },
   "source": [
    "### Creating a timelapse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZHEh5RNsiUq"
   },
   "source": [
    "Now, let's make a timelapse video for annual temperature change. To have a smooth video, we'll plot the 5-year rolling average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9VEsSzGrrwE"
   },
   "outputs": [],
   "source": [
    "tas_change_yr_rolling5 = tas_change_yr.rolling(year=5,center=True).mean().dropna('year').tas\n",
    "# Make a directory to save all the figures there:\n",
    "if not os.path.exists('./Figures_ssp585/'):\n",
    "    os.makedirs('./Figures_ssp585/')\n",
    "\n",
    "for i in range(len(tas_change_yr_rolling5)):\n",
    "    dataplot = tas_change_yr_rolling5[i,:,:]\n",
    "    # Convert 0:360 to -180:180 :\n",
    "    dataplot = dataplot.assign_coords(lon = dataplot.lon - (dataplot.lon > 180)*360)\n",
    "    dataplot = dataplot.sortby('lon', ascending=True)\n",
    "\n",
    "    fig = plt.figure(figsize=[9,5], facecolor='w')\n",
    "    # Adjust plot area (I find these by try and error until I get what I want)\n",
    "    plt.subplots_adjust(left=0.075, right=0.895, bottom=0.1, top=0.93)\n",
    "    plt.pcolormesh(dataplot.lon, dataplot.lat, dataplot, cmap='plasma', vmin=0, vmax=12)\n",
    "    plt.title(f'Near-surface air temperature change: {model} ssp585, {dataplot.year.values} vs. 1980-2010',\n",
    "              fontsize = 14)\n",
    "    plt.ylabel('Latitude', fontsize = 12)\n",
    "    plt.xlabel('Longitude', fontsize = 12)\n",
    "    # Add colorbar:\n",
    "    cax = fig.add_axes([0.91, 0.12, 0.02, 0.8])\n",
    "    cb = plt.colorbar(cax=cax, orientation='vertical', extend = 'max')\n",
    "    cb.ax.tick_params(labelsize=11)\n",
    "    cb.set_label(label='Temperature Change (°C)', color = 'k', size=13)\n",
    "    # Save and close figure:\n",
    "    plt.savefig(f'./Figures_ssp585/Fig_tasChange_{dataplot.year.values}.png', \n",
    "                format = 'png', dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mgKfNAj0s4M"
   },
   "source": [
    "Great! Now we have saved all the figures. We can use different libraries to generate a timelapse (/animation) from the figures. I will use **[openCV](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)** here, which is an amazing library for image processing and you can also leverage it for different applications in climate data analysis (e.g. spatial smoothing, applying filters or weighted kernels to emphasize on edges for feature detection/extraction, or use it for data augmentation in computer vision applications). OpenCV requires a whole tutorial on its own and I'm not going to go in details here. But again, if you intend to be an advanced user, I highly recommend working with OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg6_mPwK0sj7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joquCUwt7fo-"
   },
   "outputs": [],
   "source": [
    "files = glob.glob(f'./Figures_ssp585/Fig_tasChange*.png')\n",
    "files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_44HWtYn8JL"
   },
   "outputs": [],
   "source": [
    "img_array = []\n",
    "for filename in files:\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "    img_array.append(img)\n",
    "fps = 4\n",
    "out = cv2.VideoWriter(f'Vid_tasChange_ssp585.mp4',cv2.VideoWriter_fourcc(*'MP4V'), 4, size)\n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jX9ADX_a-0Wh"
   },
   "source": [
    "There we go! We were able to load CMIP6 data directly from Google Cloud Storage, analyze the data, generate figures, and then make a timelapse animation. Remember that after you close the Colab (or if your session is terminated for any reason), you will lose all the results. So, if you want to keep any of this result, you can either download it to your local computer or save/copy the output to your own Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TvRADx0A_7F"
   },
   "source": [
    "### Generate simple timeseries plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSuoDFctn6BM"
   },
   "source": [
    "We can define a region of interest and explore the spatial mean of the temperature change. For this example, we'll focus on the Northwest US (e.g. the Cascades):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XuY0EvaOlZBJ"
   },
   "outputs": [],
   "source": [
    "left = 236\n",
    "right = 240\n",
    "bottom = 42\n",
    "top = 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8YoKU23mW1f"
   },
   "outputs": [],
   "source": [
    "tas_NW_yr_hist = ds_hist_sel.isel(lat = (ds_hist_sel.lat>=bottom) & (ds_hist_sel.lat<=top),\n",
    "                   lon = (ds_hist_sel.lon>=left) & (ds_hist_sel.lon<=right),\n",
    "                   ).mean(['lat','lon']).drop(['bnds', 'height', 'time_bnds'])\n",
    "tas_NW_yr_ssp585 = ds_ssp585_sel.isel(lat = (ds_ssp585_sel.lat>=bottom) & (ds_ssp585_sel.lat<=top),\n",
    "                   lon = (ds_ssp585_sel.lon>=left) & (ds_ssp585_sel.lon<=right),\n",
    "                   ).mean(['lat','lon']).drop(['bnds', 'height', 'time_bnds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "R0huhwUuCYdF",
    "outputId": "1b53488e-57ca-45a7-ad4a-f403a542c159"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,5],)\n",
    "(tas_NW_yr_hist.groupby('time.year').mean().tas-273.15).plot(\n",
    "    label='historical', color='b', linewidth=2)\n",
    "(tas_NW_yr_ssp585.groupby('time.year').mean().tas-273.15).plot(\n",
    "    label='ssp585', color='r', linewidth=2)\n",
    "plt.grid()\n",
    "plt.xlim([1980,2100])\n",
    "plt.legend(fontsize=13)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.ylabel('Average Annual Temperature (°C)', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Year', fontsize=13, fontweight='bold')\n",
    "plt.title(f'Average Annual air temperature in the Cascades: {model} ssp585',\n",
    "          fontsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZleYAeL5Y76E"
   },
   "source": [
    "That concludes the advanced section of this tutorial. I hope you found it useful. Suggestions and feedbacks are appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euSytC_7Y3e5"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3DBXbcVfBL1"
   },
   "source": [
    "# 4. Advanced+ (Machine Learning):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NakWcLdAfKRt"
   },
   "source": [
    "I shared a case study more than a year ago where I used climate data to predict wildfire frequency in California. It is a relatively simple study and should be a good exercise for developing a machine learning prediction model. I have shared all the codes and explained the process in this link:\n",
    "https://www.linkedin.com/pulse/ai-climate-data-predicting-fire-frequency-california-ali-ahmadalipour/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B1QiHC-rCYy2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial_Python_Geospatial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
